{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning to solve the Taxi-v2 Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook partially follows, and uses the same illustration as \"Reinforcement Q-Learning from Scratch in Python with OpenAI Gym by LearnDataSci\" for the sake of comparison, but does not use the same code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning lets the agent use the environment's rewards to learn, the best action to take in a given state, over time.<br>\n",
    "In the Taxi environment, we have the reward table, that the agent will learn from. It does this by looking receiving a reward for taking an action in the current state, then updating a value to remember if that action was beneficial.<br>\n",
    "These values are known as Q-values and are stored in a Q-table. They map to a (state, action) combination.<br>\n",
    "A Q-value for a particular state-action combination is representative of the \"quality\" of an action taken from that state. Better Q-values imply better chances of getting greater rewards.<br>\n",
    "Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated. This is the Q-Learning algorithm.<br>\n",
    "So,\n",
    "1. The Q-table is initialized by all zeros.\n",
    "2. For each state (S), one action among all possible actions is selected.\n",
    "3. (S') is the outcome state as a result of that action (a).\n",
    "4. For all possible actions in the state (S') the one with the highest Q-value is selected.\n",
    "5. Q-table values are updated using the equation.\n",
    "6. the new state is set as the current state\n",
    "7. Once the goal is reached, end and repeat the process for the next episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The State Space is the set of all possible situations our taxi could inhabit, whereas the Action Space is the set of all the actions that the agent can take in a given state.<br>\n",
    "Since there are 5 X 5 possible taxi locations, 5 possible passenger locations (4 corresponding to RGBY with 1 additional of being in the taxi), and 4 possible destinations(corresponding to RGBY), the State Space = 5 X 5 X 5 X 4 = 500.<br>\n",
    "Since the agent can take 6 possible actions at any given state, the Action Space = 6, namely:\n",
    "0. south\n",
    "1. north\n",
    "2. east\n",
    "3. west\n",
    "4. pickup\n",
    "5. dropoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space 6\n",
      "State Space 500\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "#No. of possible actions\n",
    "action_size = env.action_space.n\n",
    "print(\"Action Space\", env.action_space.n)\n",
    "#No. of possible states\n",
    "space_size = env.action_space.n\n",
    "print(\"State Space\", env.observation_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same illustration as in the tutorial is used. Since, the taxi is at row 3, column 1, the passenger is at location 2, and the destination is location 0, the state can be encoded. This particular state will also be used to check the final solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Taxi environment is created, there is an initial Reward table that's also created. It is a matrix that has the number of states as rows and number of actions as columns. It has the structure {action: (probability, nextstate, reward, done)}.\n",
    "Since every state is in this matrix, we can see the default reward values assigned to state 328."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-table is a matrix which has a row for every state (500) and a column for every action (6). It is responsible to store score values for each (state, action) pair. It's first initialized to 0, and then values are updated after training, which helps us discover which actions lead to a better stream of rewards. It must be noted that the Q-table has the same dimensions as the reward table, but it has a completely different purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training algorithm that will update this Q-table as the agent explores the environment over thousands of episodes.\n",
    "\n",
    "First the hyperparameters are initialized:<br>\n",
    "alpha is the learning rate.<br>\n",
    "gamma is the discounting rate.<br>\n",
    "epsilon is the exploration rate. Since we want less and less exploration over time we reduce the exploration probability over time, keeping it between max_epsilon and min_epsilon.<br>\n",
    "decay_rate is the exponential decay rate for exploration probability.<br>\n",
    "Steps refers to maximum no. of steps in one episode.<br>\n",
    "\n",
    "For each epsiode, it is first decided whether to pick a random action or to exploit the already computed Q-values. This is done simply by using the epsilon value and comparing it to the random.uniform(0, 1) function, which returns an arbitrary number between 0 and 1.\n",
    "\n",
    "We execute the chosen action in the environment to obtain the new_state and the reward from performing the action. After that, we calculate the maximum Q-value for the actions corresponding to the new_state, and with that, we can easily update our Q-value to the new Q-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "import random\n",
    "\n",
    "steps = 99\n",
    "total_episodes = 100000     \n",
    "alpha = 0.618               \n",
    "gamma = 0.3                \n",
    "\n",
    "epsilon = 1.0                 \n",
    "max_epsilon = 1.0             \n",
    "min_epsilon = 0.000001            \n",
    "decay_rate = 0.001             \n",
    "\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    \n",
    "    for step in range(steps):        \n",
    "        \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(qtable[state,:]) # Exploit learned values\n",
    "        \n",
    "        \n",
    "        # Take the action and observe the outcome state and reward\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Q(old):= Q(old) + lr [R + gamma * max Q(new) - Q(old)]\n",
    "        qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * \n",
    "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        # Let new_state be state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "    \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    \n",
    "if episode % 100 == 0:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On checking the Q-Table for the above illustration, it can be observed that the maximum Q-value is for \"north\" (-1.427), so it seems that our Q-learning algorithm has effectively learned the best action to take in the particular setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.42851828,  -1.42798094,  -1.42851828,  -1.42839428,\n",
       "       -10.42839428, -10.42839428])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtable[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 100000 episodes, the obtained Q-Table can reliably be used to play the Taxi Environment, maximising the rewards and minimizing the penalties.<br>\n",
    "For 100 test epsiodes, where the maximum steps per episode are 99, we achieve a pretty good score over time of 8.84."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 12\n",
      "Score 14\n",
      "Score 7\n",
      "Score 11\n",
      "Score 10\n",
      "Score 7\n",
      "Score 15\n",
      "Score 9\n",
      "Score 7\n",
      "Score 6\n",
      "Score 5\n",
      "Score 10\n",
      "Score 9\n",
      "Score 7\n",
      "Score 13\n",
      "Score 13\n",
      "Score 5\n",
      "Score 3\n",
      "Score 10\n",
      "Score 9\n",
      "Score 8\n",
      "Score 8\n",
      "Score 15\n",
      "Score 10\n",
      "Score 11\n",
      "Score 6\n",
      "Score 12\n",
      "Score 5\n",
      "Score 9\n",
      "Score 12\n",
      "Score 12\n",
      "Score 12\n",
      "Score 5\n",
      "Score 10\n",
      "Score 11\n",
      "Score 10\n",
      "Score 8\n",
      "Score 7\n",
      "Score 6\n",
      "Score 10\n",
      "Score 10\n",
      "Score 3\n",
      "Score 5\n",
      "Score 9\n",
      "Score 8\n",
      "Score 9\n",
      "Score 11\n",
      "Score 9\n",
      "Score 7\n",
      "Score 9\n",
      "Score 8\n",
      "Score 9\n",
      "Score 7\n",
      "Score 6\n",
      "Score 10\n",
      "Score 8\n",
      "Score 11\n",
      "Score 3\n",
      "Score 8\n",
      "Score 9\n",
      "Score 5\n",
      "Score 8\n",
      "Score 10\n",
      "Score 13\n",
      "Score 10\n",
      "Score 14\n",
      "Score 12\n",
      "Score 7\n",
      "Score 8\n",
      "Score 11\n",
      "Score 8\n",
      "Score 13\n",
      "Score 6\n",
      "Score 6\n",
      "Score 6\n",
      "Score 9\n",
      "Score 7\n",
      "Score 12\n",
      "Score 10\n",
      "Score 6\n",
      "Score 6\n",
      "Score 6\n",
      "Score 5\n",
      "Score 7\n",
      "Score 8\n",
      "Score 11\n",
      "Score 10\n",
      "Score 5\n",
      "Score 10\n",
      "Score 7\n",
      "Score 9\n",
      "Score 11\n",
      "Score 8\n",
      "Score 14\n",
      "Score 6\n",
      "Score 13\n",
      "Score 12\n",
      "Score 8\n",
      "Score 6\n",
      "Score 13\n",
      "Score over time: 8.84\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "\n",
    "env.reset()\n",
    "rewards = []\n",
    "test_episodes = 100\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    for step in range(steps):\n",
    "        \n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            print (\"Score\", total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "print (\"Score over time: \" +  str(sum(rewards)/test_episodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
